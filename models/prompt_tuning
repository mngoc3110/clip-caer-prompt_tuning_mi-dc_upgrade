# models/prompt_tuning.py
import torch
import torch.nn as nn
import clip

class PromptLearner(nn.Module):
    """
    Prompt tuning text-side cho CLIP:
    - prompt template: "a student showing {desc} in learning state."
    - learnable context tokens <ctx> ch√®n v√†o gi·ªØa "showing" v√† description
    """

    def __init__(self, clip_model, classnames, n_ctx=4, ctx_init=None, device="cuda"):
        super().__init__()
        self.clip = clip_model
        self.dtype = clip_model.dtype
        self.device = device

        # classnames: list c√°c m√¥ t·∫£ ng·ªØ nghƒ©a (descriptor)
        # v√≠ d·ª•: ["The student looks neutral and calm while learning.", ...]
        self.classnames = classnames
        self.n_cls = len(classnames)
        self.n_ctx = n_ctx

        # Template hard
        self.template = "a student showing {} in learning state."

        # ===== 1. Kh·ªüi t·∫°o context =====
        if ctx_init is not None:
            # b·∫°n c√≥ th·ªÉ d√πng m·ªôt chu·ªói nh∆∞ "a person in classroom" ƒë·ªÉ init
            print(f"Initializing context with: {ctx_init}")
            with torch.no_grad():
                # tokenize m·ªôt prompt c√≥ ctx_init
                tokenized = clip.tokenize(self.template.format(ctx_init)).to(device)
                # (1,77,d)
                embedding = self.clip.token_embedding(tokenized).type(self.dtype)
            # l·∫•y n_ctx token ƒë·∫ßu (b·ªè SOS) l√†m init
            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]
        else:
            # random init
            ctx_vectors = torch.empty(self.n_ctx, self.clip.text_projection.shape[0])
            nn.init.normal_(ctx_vectors, std=0.02)
        self.ctx = nn.Parameter(ctx_vectors)  # (n_ctx, d)

        # ===== 2. Token ho√° c√°c prompt c·ª©ng cho t·ª´ng l·ªõp =====
        # v√≠ d·ª•: "a student showing The student looks neutral ... in learning state."
        prompts = [self.template.format(name) for name in classnames]
        print("\nüîé M·ªôt v√†i prompt g·ªëc (template):")
        for i in range(min(3, len(prompts))):
            print("  ", prompts[i])

        # (C, 77)
        tokenized = clip.tokenize(prompts).to(device)
        self.register_buffer("tokenized_prompts", tokenized)

    def construct_prompts(self, ctx):
        """
        ctx: (n_ctx,d)
        Tr·∫£ v·ªÅ token embedding (C,77,d) v·ªõi ph·∫ßn context ƒë∆∞·ª£c thay b·∫±ng ctx.
        """
        ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)  # (C,n_ctx,d)
        tokenized = self.tokenized_prompts  # (C,77)
        with torch.no_grad():
            embedding = self.clip.token_embedding(tokenized).type(self.dtype)  # (C,77,d)

        # SOS = token ƒë·∫ßu
        sos = embedding[:, 0:1, :]           # (C,1,d)
        rest = embedding[:, 1 + self.n_ctx :, :]  # ph·∫ßn c√≤n l·∫°i sau context (C, 77-1-n_ctx, d)

        # prompt = [SOS] + [CTX1..CTXn] + [rest...]
        prompts = torch.cat([sos, ctx, rest], dim=1)  # (C,1+n_ctx+len_rest,d)
        return prompts

    def forward(self):
        return self.construct_prompts(self.ctx)

    def encode_text(self, clip_model):
        """
        Encode text prompt ‚Üí text features (C,d)
        """
        prompts = self()               # (C,77,d)
        x = prompts

        # Gi·ªëng code encode_text c·ªßa CLIP nh∆∞ng d√πng prompts ƒë√£ thay ctx
        x = x + clip_model.positional_embedding.type(self.dtype)
        x = x.permute(1, 0, 2)        # (77,C,d)
        x = clip_model.transformer(x)
        x = x.permute(1, 0, 2)        # (C,77,d)
        x = clip_model.ln_final(x).type(self.dtype)

        # l·∫•y token <EOT> v·ªã tr√≠ c·ªßa max token_id
        tokenized = self.tokenized_prompts
        eos_ind = tokenized.argmax(dim=-1)   # (C,)
        x = x[torch.arange(x.shape[0]), eos_ind] @ clip_model.text_projection  # (C,d)

        return x